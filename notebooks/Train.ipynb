{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5780cc1-d1df-4114-8a5b-4fbd8cf3eb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.model import Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629b92c-dc97-4e2d-a8d3-1a2b3378027a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2864119-3b4e-42e0-9414-b76a9bb7c845",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c869fb-3038-4502-923d-10a6184827e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3492, 16) (748, 16)\n"
     ]
    }
   ],
   "source": [
    "DATA_PROCESSED = Path(\"../data/processed\")\n",
    "train_df = pd.read_csv(DATA_PROCESSED/\"train.csv\", header = None)\n",
    "val_df   = pd.read_csv(DATA_PROCESSED/\"validation.csv\", header = None)\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac002256-58b3-4529-9de3-495e8deecaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data to: s3://excalibur-ai-model/xgboost/train/train.csv\n",
      "Uploaded training data to: s3://excalibur-ai-model/xgboost/validation/validation.csv\n"
     ]
    }
   ],
   "source": [
    "prefix = 'xgboost'\n",
    "bucket_name = 'excalibur-ai-model'\n",
    "train_key = f'{prefix}/train/train.csv'\n",
    "validation_key = f'{prefix}/validation/validation.csv'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(DATA_PROCESSED/'train.csv', bucket_name, train_key)\n",
    "s3.upload_file(DATA_PROCESSED/'validation.csv', bucket_name, validation_key)\n",
    "\n",
    "s3_train_path = f's3://{bucket_name}/{train_key}'\n",
    "s3_validation_path = f's3://{bucket_name}/{validation_key}'\n",
    "\n",
    "print(f\"Uploaded training data to: {s3_train_path}\")\n",
    "print(f\"Uploaded training data to: {s3_validation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12131e9-2305-48b1-b5c9-574884e9a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name = 'us-east-1')\n",
    "session = sagemaker.Session(boto_session = boto_session)\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee6dd7-4ac5-4546-ba4c-c090bd1bb212",
   "metadata": {},
   "source": [
    "Instantiate xgboost container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e01065-c1a1-4947-a715-abfaf4653e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "container = image_uris.retrieve(\n",
    "    framework = \"xgboost\",\n",
    "    region = \"us-east-1\",\n",
    "    version = \"1.7-1\"  \n",
    ")\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a60579-87c1-4695-98e0-30280c51550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'xgboost'\n",
    "bucket_name = 'excalibur-ai-model'\n",
    "xgb = Estimator(\n",
    "    image_uri = container,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.large',\n",
    "    output_path = f's3://{bucket_name}/{prefix}/output',\n",
    "    sagemaker_session = session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825908ae-827a-45c6-8598-57e6b297e1a1",
   "metadata": {},
   "source": [
    "Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a09582-2137-45e7-a208-e2cd53f75894",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(\n",
    "    objective = \"binary:logistic\",\n",
    "    num_round = 500, #number of boosting iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f880f3-470a-4092-940e-35db994dd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"max_depth\": IntegerParameter(5, 10), \n",
    "    # Controls the maximum depth of each decision tree.\n",
    "    # Larger values make the model more complex, but risk overfitting.\n",
    "    \n",
    "    \"eta\": ContinuousParameter(0.01, 0.25), \n",
    "    # Learning rate (shrinkage). Smaller values make learning slower but more precise.\n",
    "    # Often paired with more boosting rounds.\n",
    "\n",
    "    \"gamma\": ContinuousParameter(0.0, 5.0),\n",
    "    # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "    # The larger, the more conservative the algorithm is.\n",
    "    \n",
    "    \"min_child_weight\": IntegerParameter(1, 10), \n",
    "    # Minimum sum of instance weight (hessian) needed in a child.\n",
    "    # Larger values can make the model more conservative (less complex splits).\n",
    "    \n",
    "    \"subsample\": ContinuousParameter(0.8, 1.0), \n",
    "    # Fraction of the training data used to grow each tree.\n",
    "    # Prevents overfitting; lower values add randomness.\n",
    "\n",
    "    \"colsample_bytree\": ContinuousParameter(0.2, 1.0)\n",
    "    # Subsample ratio of columns when constructing each tree.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02954c1e-c3ca-4272-bc6f-45b70e2e5864",
   "metadata": {},
   "source": [
    "Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cc133e5-c7ee-4f73-ade5-9ef36be56cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator = xgb,\n",
    "    objective_metric_name = \"validation:auc\", # How well the model separates the positive cases from the negative cases \n",
    "    hyperparameter_ranges = hyperparameter_ranges,\n",
    "    metric_definitions = [{\n",
    "        \"Name\": \"validation:auc\",\n",
    "        \"Regex\": \".*\\\\[.*\\\\].*validation-auc:([0-9\\\\.]+)\"\n",
    "    }],\n",
    "    max_jobs = 20, # Different training exercises that will be performed during the hyperparameter tuning.\n",
    "    objective_type = \"Maximize\" # The highest value of the target metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228b1cf-c0ed-4f09-8843-b4f08a057f09",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "684552c5-44dd-4199-852f-36e71442e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "s3_train_path = 'excalibur-ai-model/xgboost/train/train.csv'\n",
    "s3_validation_path = 'excalibur-ai-model/xgboost/validation/validation.csv'\n",
    "\n",
    "tuner.fit({\n",
    "    \"train\": TrainingInput(f's3://{s3_train_path}', content_type = \"text/csv\"),\n",
    "    \"validation\": TrainingInput(f's3://{s3_validation_path}', content_type = \"text/csv\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa269b-1766-4806-a3cf-349392dd536e",
   "metadata": {},
   "source": [
    "Show parameters set by best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d61256-b7fa-4562-a492-ea88cb523f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2026-02-23 13:14:07 Starting - Found matching resource for reuse\n",
      "2026-02-23 13:14:07 Downloading - Downloading the training image\n",
      "2026-02-23 13:14:07 Training - Training image download completed. Training in progress.\n",
      "2026-02-23 13:14:07 Uploading - Uploading generated training model\n",
      "2026-02-23 13:14:07 Completed - Resource retained for reuse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': 'validation:auc',\n",
       " 'colsample_bytree': '0.4405575686369554',\n",
       " 'eta': '0.24021326137376825',\n",
       " 'gamma': '4.063165316339813',\n",
       " 'max_depth': '8',\n",
       " 'min_child_weight': '1',\n",
       " 'num_round': '500',\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': '0.8254940017132779'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = tuner.best_estimator()\n",
    "best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b845f32-1dcf-4fe1-b38e-63e86a0a7b95",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbed15e-287a-48e4-8349-35a747f36654",
   "metadata": {},
   "source": [
    "Instantiate predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f698bcf-039f-4e6d-abc4-e19d7f460c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "test_predictor = best_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa3f580-b68e-4395-81f9-b21735fe5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictor.serializer = CSVSerializer()\n",
    "test_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263826b1-edb5-4fc1-b655-ee6d71e00633",
   "metadata": {},
   "source": [
    "Read test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3651d3d1-273b-4753-80ec-6f60edbac0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROCESSED = Path(\"../data/processed\")\n",
    "X_test_df = pd.read_csv(DATA_PROCESSED/\"X_test.csv\")\n",
    "y_test_df  = pd.read_csv(DATA_PROCESSED/\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b0465a-5889-483d-95b6-2ac2b8ed19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = X_test_df.to_csv(header=False, index=False)\n",
    "test_result = test_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cf307-d0bf-4e0c-9336-34038c7cf3a0",
   "metadata": {},
   "source": [
    "Get predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d3ecdaa-f223-47ec-a27a-a85182da8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"predictions\" in test_result:\n",
    "    test_predictions = np.array([p[\"score\"] for p in test_result[\"predictions\"]])\n",
    "test_predicted_labels = (test_predictions >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c30208-4847-4dac-8907-37ff740fa407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones: (749,)\n",
      "Reales: (749,)\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test_df.iloc[:, 0].values\n",
    "print(\"Predicciones:\", test_predicted_labels.shape)\n",
    "print(\"Reales:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee2df5-2409-4949-b122-3217a4e1ce59",
   "metadata": {},
   "source": [
    "Compare and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c56cbd8a-3f52-47c4-9bfd-0b73995170ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7770360480640854\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, test_predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc2223-22ac-4064-8252-52f12a82b968",
   "metadata": {},
   "source": [
    "## Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca268f-57d9-4310-b01e-1499b5171b44",
   "metadata": {},
   "source": [
    "Download trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f3af6b-bae3-4e1f-a34f-5d505e63a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "MODEL_PATH = PROJECT_ROOT / \"model\"\n",
    "local_model_path = MODEL_PATH / \"tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc355a9-9cb6-4e25-b826-55f7c4188256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sagemaker-user/model/tmp/model.tar.gz']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s3_uri = best_estimator.model_data\n",
    "\n",
    "session.download_data(\n",
    "    path = str(local_model_path),\n",
    "    bucket = model_s3_uri.split(\"/\")[2],\n",
    "    key_prefix = \"/\".join(model_s3_uri.split(\"/\")[3:])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7a6c0-87fe-4b9f-a7fa-80957795f5b7",
   "metadata": {},
   "source": [
    "Extract model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5f4d869-39d3-4489-bf07-de4e102c1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183092/18250783.py:2: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path = local_model_path)\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open(local_model_path / \"model.tar.gz\") as tar:\n",
    "    tar.extractall(path = local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac4b79-a32c-44e2-9673-fd08321a1ce1",
   "metadata": {},
   "source": [
    "Get encoder, sacler and columns from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff0007ea-72cc-407f-af3e-0a3540d05b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_PATH = Path(\"../model/artifacts\")\n",
    "for file in [\n",
    "    \"onehot_encoder.joblib\",\n",
    "    \"scaler.joblib\",\n",
    "    \"feature_columns.joblib\"\n",
    "]:\n",
    "    shutil.copy(\n",
    "        ARTIFACTS_PATH / file,\n",
    "        local_model_path / file\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e0cded-eb8d-4a3f-9d2d-2a045025f575",
   "metadata": {},
   "source": [
    "Create inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a40a496-1e1c-4b18-8bac-18401483a10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = \"/opt/ml/model\"\n",
    "\n",
    "MODEL_FILE = os.path.join(MODEL_PATH, \"xgboost-model\")\n",
    "ENCODER_FILE = os.path.join(MODEL_PATH, \"onehot_encoder.joblib\")\n",
    "SCALER_FILE = os.path.join(MODEL_PATH, \"scaler.joblib\")\n",
    "FEATURE_COLUMNS_FILE = os.path.join(MODEL_PATH, \"feature_columns.joblib\")\n",
    "\n",
    "# Load model and atifacts\n",
    "def model_fn(model_dir):\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(MODEL_FILE)\n",
    "\n",
    "    encoder = joblib.load(ENCODER_FILE)\n",
    "    scaler = joblib.load(SCALER_FILE)\n",
    "    feature_columns = joblib.load(FEATURE_COLUMNS_FILE)\n",
    "\n",
    "   # Assure version compatibility\n",
    "    \n",
    "    # Case .sparse vs .sparse_output\n",
    "    if hasattr(encoder, 'sparse_output') and not hasattr(encoder, 'sparse'):\n",
    "        encoder.sparse = encoder.sparse_output\n",
    "    elif hasattr(encoder, 'sparse') and not hasattr(encoder, 'sparse_output'):\n",
    "        encoder.sparse_output = encoder.sparse\n",
    "\n",
    "    encoder.sparse = False\n",
    "    if hasattr(encoder, 'sparse_output'):\n",
    "        encoder.sparse_output = False\n",
    "\n",
    "    # Case get_feature_names_out (v1.0+) vs get_feature_names (v0.24-)\n",
    "    if not hasattr(encoder, 'get_feature_names_out'):\n",
    "        if hasattr(encoder, 'get_feature_names'):\n",
    "            encoder.get_feature_names_out = encoder.get_feature_names\n",
    "        else:\n",
    "            encoder.get_feature_names_out = lambda: [f\"cat_{i}\" for i in range(encoder.categories_[0].size)]\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"model\": booster,\n",
    "        \"encoder\": encoder,\n",
    "        \"scaler\": scaler,\n",
    "        \"feature_columns\": feature_columns\n",
    "    }\n",
    "\n",
    "# Transform input\n",
    "def input_fn(request_body, request_content_type):\n",
    "\n",
    "    if request_content_type == 'application/json':\n",
    "        data = json.loads(request_body)\n",
    "        df = pd.DataFrame([data])\n",
    "        df.columns = [\n",
    "            \"did_you_get_injured_byaslip_or_fall_accident\",\n",
    "            \"did_you_have_an_accident_at_work\",\n",
    "            \"how_you_were_involved\",\n",
    "            \"days_since_accident\",\n",
    "            \"state_accident_occur\",\n",
    "            \"were_you_affected_by_possible_malpractice\",\n",
    "            \"were_you_involved_in_an_automobile_accident\"\n",
    "        ]\n",
    "\n",
    "        df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        df = df.replace('None', np.nan)\n",
    "\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "# Predict\n",
    "def predict_fn(input_data, model_artifacts):\n",
    "    df = input_data.copy()\n",
    "    print(\"Columns received:\", df.columns)\n",
    "    \n",
    "    booster = model_artifacts[\"model\"]\n",
    "    encoder = model_artifacts[\"encoder\"]\n",
    "    scaler = model_artifacts[\"scaler\"]\n",
    "    feature_columns = model_artifacts[\"feature_columns\"]\n",
    "\n",
    "    # Scale numeric column\n",
    "    df[\"days_since_accident\"] = scaler.transform(\n",
    "        df[[\"days_since_accident\"]]\n",
    "    )\n",
    "    \n",
    "    # Convert boolean to 0/1\n",
    "    boolean_cols = [\n",
    "        \"did_you_get_injured_byaslip_or_fall_accident\",\n",
    "        \"did_you_have_an_accident_at_work\",\n",
    "        \"were_you_affected_by_possible_malpractice\",\n",
    "        \"were_you_involved_in_an_automobile_accident\"\n",
    "    ]\n",
    "\n",
    "    for col in boolean_cols:\n",
    "        df[col] = df[col].map({True: 1, False: 0})\n",
    "    \n",
    "    # Validate state\n",
    "    valid_states = [\"New York\", \"New Jersey\"]\n",
    "    state_value = df.loc[0, \"state_accident_occur\"]\n",
    "    \n",
    "    if state_value not in valid_states:\n",
    "        raise ValueError(\n",
    "            f\"Invalid state_accident_occur: '{state_value}'. \"\n",
    "            f\"Allowed values are {valid_states}.\"\n",
    "        )\n",
    "\n",
    "    # Encode categorical columns\n",
    "    df[\"how_you_were_involved\"] = df[\"how_you_were_involved\"].fillna(\"Not_involved\")\n",
    "    categorical_cols = [\"how_you_were_involved\", \"state_accident_occur\"]    \n",
    "    encoded = encoder.transform(df[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded,\n",
    "        columns=encoder.get_feature_names_out(categorical_cols),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Concat all columns\n",
    "    final_df = pd.concat(\n",
    "        [df[[\"days_since_accident\"]], encoded_df],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "   # Align columns with feature_columns\n",
    "    final_df = final_df.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "    # Convert to DMatrix and predict\n",
    "    dmatrix = xgb.DMatrix(final_df.values)\n",
    "    predictions = booster.predict(dmatrix)\n",
    "    return predictions\n",
    "\n",
    "# Output\n",
    "def output_fn(predictions, content_type):\n",
    "\n",
    "    if content_type == \"application/json\":\n",
    "        return json.dumps({\n",
    "            \"predictions\": predictions.tolist()\n",
    "        }), content_type\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a6a27-fc9a-4e05-92bf-3153c199e3d3",
   "metadata": {},
   "source": [
    "Repackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6935342e-a723-41b9-a70f-2b05b2a4968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_BUILD_PATH = Path(\"../model/builds\")\n",
    "MODEL_BUILD_PATH.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "new_model_tar = MODEL_BUILD_PATH / \"model_complete.tar.gz\"\n",
    "\n",
    "with tarfile.open(new_model_tar, \"w:gz\") as tar:\n",
    "    for file in local_model_path.iterdir():\n",
    "        tar.add(file, arcname = file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130a780-2b91-42c9-ae13-6e6d5d276ab3",
   "metadata": {},
   "source": [
    "Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b4dab44-0d08-4e72-987e-3da636d75157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://excalibur-ai-model/xgboost/excalibur-aimodel/model_complete.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_complete_s3 = session.upload_data(\n",
    "    path = str(new_model_tar),\n",
    "    bucket = bucket_name,\n",
    "    key_prefix = \"xgboost/excalibur-aimodel\"\n",
    ")\n",
    "\n",
    "print(model_complete_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d84e89-22f1-4ea9-aeca-9540f8c9d4af",
   "metadata": {},
   "source": [
    "Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81fa6238-05a1-40db-8bf6-d7308da40836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating endpoint 'xgboost-excalibur' using model_complete.tar.gz ...\n",
      "------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"xgboost-excalibur\"\n",
    "region = \"us-east-1\"\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "existing_endpoints = sm_client.list_endpoints(NameContains = endpoint_name)[\"Endpoints\"]\n",
    "endpoint_names = [ep[\"EndpointName\"] for ep in existing_endpoints]\n",
    "\n",
    "if endpoint_name in endpoint_names:\n",
    "    print(f\"Endpoint '{endpoint_name}' already exists. Reusing it.\")\n",
    "    \n",
    "    predictor = Predictor(\n",
    "        endpoint_name = endpoint_name,\n",
    "        sagemaker_session = session\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"Creating endpoint '{endpoint_name}' using model_complete.tar.gz ...\")\n",
    "\n",
    "    model = Model(\n",
    "        image_uri = best_estimator.image_uri,\n",
    "        model_data = model_complete_s3,\n",
    "        role = role,\n",
    "        entry_point = \"inference.py\",\n",
    "        sagemaker_session = session\n",
    "    )\n",
    "\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count = 1,\n",
    "        instance_type = \"ml.m5.large\",\n",
    "        endpoint_name = endpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed882092-f58e-461f-b96f-822fcf9efb3b",
   "metadata": {},
   "source": [
    "WARNING: always delete endpoint and endpoint configuration after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16d67146-f0af-43f4-a5d0-7c6fc2d53056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ef2a5060-0e26-422d-a9ff-fb39b14e9be9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ef2a5060-0e26-422d-a9ff-fb39b14e9be9',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Mon, 23 Feb 2026 22:56:27 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", region_name=\"us-east-1\")\n",
    "#sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "#sm_client.delete_endpoint_config(EndpointConfigName=\"xgboost-excalibur\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
